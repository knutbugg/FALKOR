{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.charting_tools import Charting\n",
    "from helpers.data_processing import add_ti\n",
    "from BookWorm import BookWorm, BinanceWrapper\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "worm = BookWorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "candles = worm.historical_candles(start_time='January 1 2019', end_time='February 1 2019', api_wrapper=BinanceWrapper('5lJ0uGit9PuUxHka3hBWhPmsi7dWyxEwvEntUZFKmm0xfNz3VjHWi5WSr5W1VBJV',\n",
    "                                                      'BFWVs8ko7Cd4sjdQ9amGJTnToGWy9TbQWIjeorSCj23FGiwFaknzkgLPcrgWrxsw'), \n",
    "                  symbol='ETHBTC', interval='1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "candles = add_ti(candles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_candles(df, num_rows=30, step=10):\n",
    "    \"\"\"Split a DataFrame of candlestick data into a list of smaller DataFrames each with num_rows rows\"\"\"\n",
    "    \n",
    "    slices = []\n",
    "    \n",
    "    for row_i in range(0, df.shape[0] - num_rows, step):\n",
    "        small_df = df.iloc[row_i:row_i+num_rows, :]\n",
    "        slices.append(small_df)\n",
    "        \n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_returns(df, num_rows=30, num_into_fut=5, step=10):\n",
    "    labels = []\n",
    "    \n",
    "    for row_i in range(0, df.shape[0] - num_rows - num_into_fut, step):\n",
    "        # skip all iterations while row_i < num_rows since nothing yet to create a label for\n",
    "        if row_i <= num_rows: continue\n",
    "        \n",
    "        vf, vi = df['close'][row_i+num_into_fut], df['close'][row_i]\n",
    "        price_return = (vf - vi) / vi\n",
    "        labels.append(price_return)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4455"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split candles into 30 period and a label\n",
    "candles_sliced = split_candles(candles)\n",
    "labels_candles_sliced = price_returns(candles)\n",
    "# we need to remove candle slices without a label from candles_sliced\n",
    "candles_sliced = candles_sliced[len(candles_sliced)-len(labels_candles_sliced):]\n",
    "\n",
    "assert len(candles_sliced) == len(labels_candles_sliced)\n",
    "len(candles_sliced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_charts(candles_sliced, save_path):\n",
    "    \"\"\"Create a chart image for each in sliced_candles and return a list of paths to those images\"\"\"\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    i = 0\n",
    "    paths_to_images = []\n",
    "    for small_df in tqdm(candles_sliced):\n",
    "        chart = Charting(small_df, 'time', 'close')\n",
    "        \n",
    "        path = save_path + 'chart_{}.png'.format(i)\n",
    "        chart.chart_to_image(path)\n",
    "        paths_to_images.append(path)\n",
    "        i += 1\n",
    "    return paths_to_images        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_to_images = create_charts(candles_sliced, \"images/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_to_images = [ 'images/chart_{}.png'.format(i) for i in range(len(candles_sliced)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=224x224 at 0x7F76A9667860>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open(paths_to_images[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_series(ser):\n",
    "    return (ser-ser.min())/(ser.max()-ser.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(566841, 11)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try: candles = candles.drop('time', axis=1)\n",
    "except: pass\n",
    "candles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>sma20</th>\n",
       "      <th>macd</th>\n",
       "      <th>obv</th>\n",
       "      <th>bb20_low</th>\n",
       "      <th>bb20_mid</th>\n",
       "      <th>bb20_up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.294695</td>\n",
       "      <td>0.294816</td>\n",
       "      <td>0.295438</td>\n",
       "      <td>0.294691</td>\n",
       "      <td>0.007892</td>\n",
       "      <td>0.297465</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.724619</td>\n",
       "      <td>0.298127</td>\n",
       "      <td>0.297465</td>\n",
       "      <td>0.294494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.294988</td>\n",
       "      <td>0.294634</td>\n",
       "      <td>0.294983</td>\n",
       "      <td>0.294267</td>\n",
       "      <td>0.010340</td>\n",
       "      <td>0.297578</td>\n",
       "      <td>0.957259</td>\n",
       "      <td>0.724342</td>\n",
       "      <td>0.298357</td>\n",
       "      <td>0.297578</td>\n",
       "      <td>0.294490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.294766</td>\n",
       "      <td>0.294846</td>\n",
       "      <td>0.293143</td>\n",
       "      <td>0.294024</td>\n",
       "      <td>0.016392</td>\n",
       "      <td>0.297686</td>\n",
       "      <td>0.917540</td>\n",
       "      <td>0.723903</td>\n",
       "      <td>0.298665</td>\n",
       "      <td>0.297686</td>\n",
       "      <td>0.294401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.293887</td>\n",
       "      <td>0.293807</td>\n",
       "      <td>0.292859</td>\n",
       "      <td>0.293479</td>\n",
       "      <td>0.007877</td>\n",
       "      <td>0.297774</td>\n",
       "      <td>0.880663</td>\n",
       "      <td>0.723693</td>\n",
       "      <td>0.298952</td>\n",
       "      <td>0.297774</td>\n",
       "      <td>0.294294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.293281</td>\n",
       "      <td>0.293565</td>\n",
       "      <td>0.293355</td>\n",
       "      <td>0.293479</td>\n",
       "      <td>0.010889</td>\n",
       "      <td>0.297817</td>\n",
       "      <td>0.846456</td>\n",
       "      <td>0.723693</td>\n",
       "      <td>0.299108</td>\n",
       "      <td>0.297817</td>\n",
       "      <td>0.294225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.293271</td>\n",
       "      <td>0.294181</td>\n",
       "      <td>0.293699</td>\n",
       "      <td>0.293772</td>\n",
       "      <td>0.011751</td>\n",
       "      <td>0.297832</td>\n",
       "      <td>0.814750</td>\n",
       "      <td>0.724007</td>\n",
       "      <td>0.299157</td>\n",
       "      <td>0.297832</td>\n",
       "      <td>0.294206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.293786</td>\n",
       "      <td>0.294382</td>\n",
       "      <td>0.294073</td>\n",
       "      <td>0.294034</td>\n",
       "      <td>0.017002</td>\n",
       "      <td>0.297848</td>\n",
       "      <td>0.785378</td>\n",
       "      <td>0.724462</td>\n",
       "      <td>0.299204</td>\n",
       "      <td>0.297848</td>\n",
       "      <td>0.294193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.294604</td>\n",
       "      <td>0.294645</td>\n",
       "      <td>0.294781</td>\n",
       "      <td>0.294701</td>\n",
       "      <td>0.008270</td>\n",
       "      <td>0.297926</td>\n",
       "      <td>0.758183</td>\n",
       "      <td>0.724683</td>\n",
       "      <td>0.299497</td>\n",
       "      <td>0.297926</td>\n",
       "      <td>0.294060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.294695</td>\n",
       "      <td>0.294826</td>\n",
       "      <td>0.294821</td>\n",
       "      <td>0.294620</td>\n",
       "      <td>0.015599</td>\n",
       "      <td>0.298031</td>\n",
       "      <td>0.733012</td>\n",
       "      <td>0.724266</td>\n",
       "      <td>0.299850</td>\n",
       "      <td>0.298031</td>\n",
       "      <td>0.293920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.294634</td>\n",
       "      <td>0.294463</td>\n",
       "      <td>0.293527</td>\n",
       "      <td>0.293984</td>\n",
       "      <td>0.018777</td>\n",
       "      <td>0.298077</td>\n",
       "      <td>0.709726</td>\n",
       "      <td>0.723763</td>\n",
       "      <td>0.299970</td>\n",
       "      <td>0.298077</td>\n",
       "      <td>0.293893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.293998</td>\n",
       "      <td>0.295351</td>\n",
       "      <td>0.294478</td>\n",
       "      <td>0.294479</td>\n",
       "      <td>0.010228</td>\n",
       "      <td>0.298022</td>\n",
       "      <td>0.688200</td>\n",
       "      <td>0.724037</td>\n",
       "      <td>0.299875</td>\n",
       "      <td>0.298022</td>\n",
       "      <td>0.293878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.294493</td>\n",
       "      <td>0.294836</td>\n",
       "      <td>0.294892</td>\n",
       "      <td>0.294438</td>\n",
       "      <td>0.007775</td>\n",
       "      <td>0.297997</td>\n",
       "      <td>0.668306</td>\n",
       "      <td>0.723829</td>\n",
       "      <td>0.299847</td>\n",
       "      <td>0.297997</td>\n",
       "      <td>0.293858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.294392</td>\n",
       "      <td>0.295502</td>\n",
       "      <td>0.295084</td>\n",
       "      <td>0.294671</td>\n",
       "      <td>0.010004</td>\n",
       "      <td>0.297957</td>\n",
       "      <td>0.649926</td>\n",
       "      <td>0.724097</td>\n",
       "      <td>0.299815</td>\n",
       "      <td>0.297957</td>\n",
       "      <td>0.293809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.294665</td>\n",
       "      <td>0.295068</td>\n",
       "      <td>0.294842</td>\n",
       "      <td>0.295458</td>\n",
       "      <td>0.009563</td>\n",
       "      <td>0.297925</td>\n",
       "      <td>0.632949</td>\n",
       "      <td>0.724353</td>\n",
       "      <td>0.299804</td>\n",
       "      <td>0.297925</td>\n",
       "      <td>0.293757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.294372</td>\n",
       "      <td>0.295240</td>\n",
       "      <td>0.295105</td>\n",
       "      <td>0.295489</td>\n",
       "      <td>0.007541</td>\n",
       "      <td>0.297878</td>\n",
       "      <td>0.617268</td>\n",
       "      <td>0.724554</td>\n",
       "      <td>0.299907</td>\n",
       "      <td>0.297878</td>\n",
       "      <td>0.293563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.295503</td>\n",
       "      <td>0.295099</td>\n",
       "      <td>0.295135</td>\n",
       "      <td>0.294388</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.297858</td>\n",
       "      <td>0.602789</td>\n",
       "      <td>0.724330</td>\n",
       "      <td>0.299944</td>\n",
       "      <td>0.297858</td>\n",
       "      <td>0.293488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.294402</td>\n",
       "      <td>0.294524</td>\n",
       "      <td>0.294660</td>\n",
       "      <td>0.294842</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>0.297767</td>\n",
       "      <td>0.589431</td>\n",
       "      <td>0.724522</td>\n",
       "      <td>0.300043</td>\n",
       "      <td>0.297767</td>\n",
       "      <td>0.293212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.294857</td>\n",
       "      <td>0.294756</td>\n",
       "      <td>0.294751</td>\n",
       "      <td>0.294509</td>\n",
       "      <td>0.009219</td>\n",
       "      <td>0.297748</td>\n",
       "      <td>0.577107</td>\n",
       "      <td>0.724275</td>\n",
       "      <td>0.300054</td>\n",
       "      <td>0.297748</td>\n",
       "      <td>0.293165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.294059</td>\n",
       "      <td>0.294120</td>\n",
       "      <td>0.293830</td>\n",
       "      <td>0.293136</td>\n",
       "      <td>0.011267</td>\n",
       "      <td>0.297697</td>\n",
       "      <td>0.565743</td>\n",
       "      <td>0.723974</td>\n",
       "      <td>0.300094</td>\n",
       "      <td>0.297697</td>\n",
       "      <td>0.293025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.293251</td>\n",
       "      <td>0.293293</td>\n",
       "      <td>0.293395</td>\n",
       "      <td>0.293560</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>0.297580</td>\n",
       "      <td>0.555276</td>\n",
       "      <td>0.724200</td>\n",
       "      <td>0.299925</td>\n",
       "      <td>0.297580</td>\n",
       "      <td>0.292961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.293564</td>\n",
       "      <td>0.293232</td>\n",
       "      <td>0.293628</td>\n",
       "      <td>0.293620</td>\n",
       "      <td>0.005924</td>\n",
       "      <td>0.297523</td>\n",
       "      <td>0.545634</td>\n",
       "      <td>0.724358</td>\n",
       "      <td>0.299829</td>\n",
       "      <td>0.297523</td>\n",
       "      <td>0.292944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.293625</td>\n",
       "      <td>0.293686</td>\n",
       "      <td>0.293830</td>\n",
       "      <td>0.293762</td>\n",
       "      <td>0.014138</td>\n",
       "      <td>0.297469</td>\n",
       "      <td>0.536751</td>\n",
       "      <td>0.724736</td>\n",
       "      <td>0.299753</td>\n",
       "      <td>0.297469</td>\n",
       "      <td>0.292911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.293271</td>\n",
       "      <td>0.293444</td>\n",
       "      <td>0.293770</td>\n",
       "      <td>0.293600</td>\n",
       "      <td>0.008477</td>\n",
       "      <td>0.297443</td>\n",
       "      <td>0.528567</td>\n",
       "      <td>0.724510</td>\n",
       "      <td>0.299709</td>\n",
       "      <td>0.297443</td>\n",
       "      <td>0.292903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.293614</td>\n",
       "      <td>0.293444</td>\n",
       "      <td>0.291929</td>\n",
       "      <td>0.293186</td>\n",
       "      <td>0.016432</td>\n",
       "      <td>0.297422</td>\n",
       "      <td>0.521030</td>\n",
       "      <td>0.724070</td>\n",
       "      <td>0.299661</td>\n",
       "      <td>0.297422</td>\n",
       "      <td>0.292909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.292504</td>\n",
       "      <td>0.292768</td>\n",
       "      <td>0.291716</td>\n",
       "      <td>0.291298</td>\n",
       "      <td>0.013128</td>\n",
       "      <td>0.297407</td>\n",
       "      <td>0.514093</td>\n",
       "      <td>0.723719</td>\n",
       "      <td>0.299606</td>\n",
       "      <td>0.297407</td>\n",
       "      <td>0.292933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.292019</td>\n",
       "      <td>0.291618</td>\n",
       "      <td>0.290998</td>\n",
       "      <td>0.291449</td>\n",
       "      <td>0.014649</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>0.507722</td>\n",
       "      <td>0.724111</td>\n",
       "      <td>0.299002</td>\n",
       "      <td>0.297297</td>\n",
       "      <td>0.293307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.291463</td>\n",
       "      <td>0.291840</td>\n",
       "      <td>0.291201</td>\n",
       "      <td>0.291409</td>\n",
       "      <td>0.006351</td>\n",
       "      <td>0.297179</td>\n",
       "      <td>0.501869</td>\n",
       "      <td>0.723941</td>\n",
       "      <td>0.298547</td>\n",
       "      <td>0.297179</td>\n",
       "      <td>0.293520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.292140</td>\n",
       "      <td>0.292002</td>\n",
       "      <td>0.291635</td>\n",
       "      <td>0.292318</td>\n",
       "      <td>0.009698</td>\n",
       "      <td>0.297046</td>\n",
       "      <td>0.496492</td>\n",
       "      <td>0.724200</td>\n",
       "      <td>0.298137</td>\n",
       "      <td>0.297046</td>\n",
       "      <td>0.293659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.291534</td>\n",
       "      <td>0.291891</td>\n",
       "      <td>0.291716</td>\n",
       "      <td>0.291288</td>\n",
       "      <td>0.008832</td>\n",
       "      <td>0.296925</td>\n",
       "      <td>0.491545</td>\n",
       "      <td>0.723964</td>\n",
       "      <td>0.297964</td>\n",
       "      <td>0.296925</td>\n",
       "      <td>0.293591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.291837</td>\n",
       "      <td>0.292647</td>\n",
       "      <td>0.292030</td>\n",
       "      <td>0.292510</td>\n",
       "      <td>0.008917</td>\n",
       "      <td>0.296756</td>\n",
       "      <td>0.487001</td>\n",
       "      <td>0.724202</td>\n",
       "      <td>0.297615</td>\n",
       "      <td>0.296756</td>\n",
       "      <td>0.293601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566835</th>\n",
       "      <td>0.063613</td>\n",
       "      <td>0.063652</td>\n",
       "      <td>0.063882</td>\n",
       "      <td>0.063584</td>\n",
       "      <td>0.004849</td>\n",
       "      <td>0.064164</td>\n",
       "      <td>0.577966</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.066310</td>\n",
       "      <td>0.064164</td>\n",
       "      <td>0.062667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566836</th>\n",
       "      <td>0.063694</td>\n",
       "      <td>0.063662</td>\n",
       "      <td>0.064064</td>\n",
       "      <td>0.063705</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.064139</td>\n",
       "      <td>0.577977</td>\n",
       "      <td>0.008133</td>\n",
       "      <td>0.066238</td>\n",
       "      <td>0.064139</td>\n",
       "      <td>0.062687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566837</th>\n",
       "      <td>0.063683</td>\n",
       "      <td>0.063571</td>\n",
       "      <td>0.063872</td>\n",
       "      <td>0.063665</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>0.064122</td>\n",
       "      <td>0.577987</td>\n",
       "      <td>0.008053</td>\n",
       "      <td>0.066207</td>\n",
       "      <td>0.064122</td>\n",
       "      <td>0.062684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566838</th>\n",
       "      <td>0.063593</td>\n",
       "      <td>0.063702</td>\n",
       "      <td>0.063963</td>\n",
       "      <td>0.063816</td>\n",
       "      <td>0.006391</td>\n",
       "      <td>0.064102</td>\n",
       "      <td>0.577998</td>\n",
       "      <td>0.008224</td>\n",
       "      <td>0.066173</td>\n",
       "      <td>0.064102</td>\n",
       "      <td>0.062677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566839</th>\n",
       "      <td>0.063754</td>\n",
       "      <td>0.063783</td>\n",
       "      <td>0.064125</td>\n",
       "      <td>0.063856</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>0.064094</td>\n",
       "      <td>0.578008</td>\n",
       "      <td>0.008319</td>\n",
       "      <td>0.066164</td>\n",
       "      <td>0.064094</td>\n",
       "      <td>0.062671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566840</th>\n",
       "      <td>0.063885</td>\n",
       "      <td>0.064065</td>\n",
       "      <td>0.064195</td>\n",
       "      <td>0.064028</td>\n",
       "      <td>0.006088</td>\n",
       "      <td>0.064086</td>\n",
       "      <td>0.578017</td>\n",
       "      <td>0.008482</td>\n",
       "      <td>0.066160</td>\n",
       "      <td>0.064086</td>\n",
       "      <td>0.062658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566841</th>\n",
       "      <td>0.064128</td>\n",
       "      <td>0.064035</td>\n",
       "      <td>0.064438</td>\n",
       "      <td>0.064139</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.064090</td>\n",
       "      <td>0.578024</td>\n",
       "      <td>0.008535</td>\n",
       "      <td>0.066160</td>\n",
       "      <td>0.064090</td>\n",
       "      <td>0.062667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566842</th>\n",
       "      <td>0.064077</td>\n",
       "      <td>0.064015</td>\n",
       "      <td>0.064337</td>\n",
       "      <td>0.064038</td>\n",
       "      <td>0.003293</td>\n",
       "      <td>0.064101</td>\n",
       "      <td>0.578028</td>\n",
       "      <td>0.008447</td>\n",
       "      <td>0.066154</td>\n",
       "      <td>0.064101</td>\n",
       "      <td>0.062695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566843</th>\n",
       "      <td>0.064118</td>\n",
       "      <td>0.064045</td>\n",
       "      <td>0.064418</td>\n",
       "      <td>0.064170</td>\n",
       "      <td>0.002904</td>\n",
       "      <td>0.064108</td>\n",
       "      <td>0.578030</td>\n",
       "      <td>0.008525</td>\n",
       "      <td>0.066155</td>\n",
       "      <td>0.064108</td>\n",
       "      <td>0.062707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566844</th>\n",
       "      <td>0.064138</td>\n",
       "      <td>0.064096</td>\n",
       "      <td>0.064489</td>\n",
       "      <td>0.064210</td>\n",
       "      <td>0.004057</td>\n",
       "      <td>0.064125</td>\n",
       "      <td>0.578031</td>\n",
       "      <td>0.008634</td>\n",
       "      <td>0.066157</td>\n",
       "      <td>0.064125</td>\n",
       "      <td>0.062739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566845</th>\n",
       "      <td>0.064229</td>\n",
       "      <td>0.064116</td>\n",
       "      <td>0.064499</td>\n",
       "      <td>0.064119</td>\n",
       "      <td>0.002228</td>\n",
       "      <td>0.064128</td>\n",
       "      <td>0.578029</td>\n",
       "      <td>0.008574</td>\n",
       "      <td>0.066151</td>\n",
       "      <td>0.064128</td>\n",
       "      <td>0.062750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566846</th>\n",
       "      <td>0.064138</td>\n",
       "      <td>0.064086</td>\n",
       "      <td>0.064509</td>\n",
       "      <td>0.064109</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.064132</td>\n",
       "      <td>0.578026</td>\n",
       "      <td>0.008543</td>\n",
       "      <td>0.066149</td>\n",
       "      <td>0.064132</td>\n",
       "      <td>0.062760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566847</th>\n",
       "      <td>0.064128</td>\n",
       "      <td>0.064106</td>\n",
       "      <td>0.064509</td>\n",
       "      <td>0.064099</td>\n",
       "      <td>0.001703</td>\n",
       "      <td>0.064131</td>\n",
       "      <td>0.578023</td>\n",
       "      <td>0.008497</td>\n",
       "      <td>0.066149</td>\n",
       "      <td>0.064131</td>\n",
       "      <td>0.062759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566848</th>\n",
       "      <td>0.064128</td>\n",
       "      <td>0.063985</td>\n",
       "      <td>0.064418</td>\n",
       "      <td>0.064018</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.064138</td>\n",
       "      <td>0.578020</td>\n",
       "      <td>0.008451</td>\n",
       "      <td>0.066150</td>\n",
       "      <td>0.064138</td>\n",
       "      <td>0.062772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566849</th>\n",
       "      <td>0.064027</td>\n",
       "      <td>0.063944</td>\n",
       "      <td>0.064377</td>\n",
       "      <td>0.063968</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>0.064134</td>\n",
       "      <td>0.578017</td>\n",
       "      <td>0.008395</td>\n",
       "      <td>0.066151</td>\n",
       "      <td>0.064134</td>\n",
       "      <td>0.062762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566850</th>\n",
       "      <td>0.064077</td>\n",
       "      <td>0.063934</td>\n",
       "      <td>0.064216</td>\n",
       "      <td>0.063927</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>0.064131</td>\n",
       "      <td>0.578014</td>\n",
       "      <td>0.008312</td>\n",
       "      <td>0.066150</td>\n",
       "      <td>0.064131</td>\n",
       "      <td>0.062757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566851</th>\n",
       "      <td>0.063946</td>\n",
       "      <td>0.063934</td>\n",
       "      <td>0.064226</td>\n",
       "      <td>0.064018</td>\n",
       "      <td>0.005957</td>\n",
       "      <td>0.064130</td>\n",
       "      <td>0.578013</td>\n",
       "      <td>0.008472</td>\n",
       "      <td>0.066150</td>\n",
       "      <td>0.064130</td>\n",
       "      <td>0.062757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566852</th>\n",
       "      <td>0.064077</td>\n",
       "      <td>0.064015</td>\n",
       "      <td>0.064347</td>\n",
       "      <td>0.064028</td>\n",
       "      <td>0.003146</td>\n",
       "      <td>0.064135</td>\n",
       "      <td>0.578012</td>\n",
       "      <td>0.008556</td>\n",
       "      <td>0.066152</td>\n",
       "      <td>0.064135</td>\n",
       "      <td>0.062763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566853</th>\n",
       "      <td>0.064128</td>\n",
       "      <td>0.063995</td>\n",
       "      <td>0.063760</td>\n",
       "      <td>0.063927</td>\n",
       "      <td>0.016743</td>\n",
       "      <td>0.064140</td>\n",
       "      <td>0.578011</td>\n",
       "      <td>0.008108</td>\n",
       "      <td>0.066156</td>\n",
       "      <td>0.064140</td>\n",
       "      <td>0.062770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566854</th>\n",
       "      <td>0.063986</td>\n",
       "      <td>0.063854</td>\n",
       "      <td>0.064155</td>\n",
       "      <td>0.063806</td>\n",
       "      <td>0.002357</td>\n",
       "      <td>0.064147</td>\n",
       "      <td>0.578010</td>\n",
       "      <td>0.008045</td>\n",
       "      <td>0.066169</td>\n",
       "      <td>0.064147</td>\n",
       "      <td>0.062770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566855</th>\n",
       "      <td>0.063825</td>\n",
       "      <td>0.063934</td>\n",
       "      <td>0.064125</td>\n",
       "      <td>0.063907</td>\n",
       "      <td>0.005201</td>\n",
       "      <td>0.064157</td>\n",
       "      <td>0.578011</td>\n",
       "      <td>0.008184</td>\n",
       "      <td>0.066208</td>\n",
       "      <td>0.064157</td>\n",
       "      <td>0.062752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566856</th>\n",
       "      <td>0.064017</td>\n",
       "      <td>0.063874</td>\n",
       "      <td>0.064084</td>\n",
       "      <td>0.063816</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.064173</td>\n",
       "      <td>0.578013</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>0.066271</td>\n",
       "      <td>0.064173</td>\n",
       "      <td>0.062722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566857</th>\n",
       "      <td>0.063774</td>\n",
       "      <td>0.063722</td>\n",
       "      <td>0.064114</td>\n",
       "      <td>0.063725</td>\n",
       "      <td>0.001930</td>\n",
       "      <td>0.064179</td>\n",
       "      <td>0.578015</td>\n",
       "      <td>0.008091</td>\n",
       "      <td>0.066294</td>\n",
       "      <td>0.064179</td>\n",
       "      <td>0.062711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566858</th>\n",
       "      <td>0.063754</td>\n",
       "      <td>0.063682</td>\n",
       "      <td>0.064135</td>\n",
       "      <td>0.063806</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>0.064182</td>\n",
       "      <td>0.578019</td>\n",
       "      <td>0.008124</td>\n",
       "      <td>0.066310</td>\n",
       "      <td>0.064182</td>\n",
       "      <td>0.062701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566859</th>\n",
       "      <td>0.063825</td>\n",
       "      <td>0.063854</td>\n",
       "      <td>0.064195</td>\n",
       "      <td>0.063957</td>\n",
       "      <td>0.003121</td>\n",
       "      <td>0.064181</td>\n",
       "      <td>0.578023</td>\n",
       "      <td>0.008207</td>\n",
       "      <td>0.066308</td>\n",
       "      <td>0.064181</td>\n",
       "      <td>0.062702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566860</th>\n",
       "      <td>0.063916</td>\n",
       "      <td>0.063924</td>\n",
       "      <td>0.064307</td>\n",
       "      <td>0.064038</td>\n",
       "      <td>0.002764</td>\n",
       "      <td>0.064186</td>\n",
       "      <td>0.578027</td>\n",
       "      <td>0.008281</td>\n",
       "      <td>0.066320</td>\n",
       "      <td>0.064186</td>\n",
       "      <td>0.062701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566861</th>\n",
       "      <td>0.064057</td>\n",
       "      <td>0.063924</td>\n",
       "      <td>0.064388</td>\n",
       "      <td>0.063998</td>\n",
       "      <td>0.004898</td>\n",
       "      <td>0.064187</td>\n",
       "      <td>0.578029</td>\n",
       "      <td>0.008150</td>\n",
       "      <td>0.066320</td>\n",
       "      <td>0.064187</td>\n",
       "      <td>0.062701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566862</th>\n",
       "      <td>0.064017</td>\n",
       "      <td>0.063934</td>\n",
       "      <td>0.064317</td>\n",
       "      <td>0.063957</td>\n",
       "      <td>0.004241</td>\n",
       "      <td>0.064180</td>\n",
       "      <td>0.578031</td>\n",
       "      <td>0.008037</td>\n",
       "      <td>0.066322</td>\n",
       "      <td>0.064180</td>\n",
       "      <td>0.062685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566863</th>\n",
       "      <td>0.064077</td>\n",
       "      <td>0.063975</td>\n",
       "      <td>0.064317</td>\n",
       "      <td>0.063978</td>\n",
       "      <td>0.004918</td>\n",
       "      <td>0.064176</td>\n",
       "      <td>0.578032</td>\n",
       "      <td>0.008168</td>\n",
       "      <td>0.066319</td>\n",
       "      <td>0.064176</td>\n",
       "      <td>0.062680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566864</th>\n",
       "      <td>0.064098</td>\n",
       "      <td>0.064096</td>\n",
       "      <td>0.064337</td>\n",
       "      <td>0.064069</td>\n",
       "      <td>0.011441</td>\n",
       "      <td>0.064166</td>\n",
       "      <td>0.578032</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.066325</td>\n",
       "      <td>0.064166</td>\n",
       "      <td>0.062655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>566841 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            open      high       low     close    volume     sma20      macd  \\\n",
       "24      0.294695  0.294816  0.295438  0.294691  0.007892  0.297465  1.000000   \n",
       "25      0.294988  0.294634  0.294983  0.294267  0.010340  0.297578  0.957259   \n",
       "26      0.294766  0.294846  0.293143  0.294024  0.016392  0.297686  0.917540   \n",
       "27      0.293887  0.293807  0.292859  0.293479  0.007877  0.297774  0.880663   \n",
       "28      0.293281  0.293565  0.293355  0.293479  0.010889  0.297817  0.846456   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "566860  0.063916  0.063924  0.064307  0.064038  0.002764  0.064186  0.578027   \n",
       "566861  0.064057  0.063924  0.064388  0.063998  0.004898  0.064187  0.578029   \n",
       "566862  0.064017  0.063934  0.064317  0.063957  0.004241  0.064180  0.578031   \n",
       "566863  0.064077  0.063975  0.064317  0.063978  0.004918  0.064176  0.578032   \n",
       "566864  0.064098  0.064096  0.064337  0.064069  0.011441  0.064166  0.578032   \n",
       "\n",
       "             obv  bb20_low  bb20_mid   bb20_up  \n",
       "24      0.724619  0.298127  0.297465  0.294494  \n",
       "25      0.724342  0.298357  0.297578  0.294490  \n",
       "26      0.723903  0.298665  0.297686  0.294401  \n",
       "27      0.723693  0.298952  0.297774  0.294294  \n",
       "28      0.723693  0.299108  0.297817  0.294225  \n",
       "...          ...       ...       ...       ...  \n",
       "566860  0.008281  0.066320  0.064186  0.062701  \n",
       "566861  0.008150  0.066320  0.064187  0.062701  \n",
       "566862  0.008037  0.066322  0.064180  0.062685  \n",
       "566863  0.008168  0.066319  0.064176  0.062680  \n",
       "566864  0.008475  0.066325  0.064166  0.062655  \n",
       "\n",
       "[566841 rows x 11 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candles.apply(normalize_series, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 64,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.datasets import DFTimeSeriesDataset, ChartImageDataset\n",
    "from torch.utils.data import *\n",
    "# create dataloaders\n",
    "# specify the split between train_df and valid_df from the process of splitting dataset_windows \n",
    "split = 0.7\n",
    "\n",
    "s = int(len(candles_sliced) * 0.7)\n",
    "while s % params['batch_size'] != 0:\n",
    "    s += 1\n",
    "\n",
    "# create two ChartImageDatasets, split by split, for the purpose of creating a DataLoader for the specific model\n",
    "train_ds_cnn = ChartImageDataset(paths_to_images[:s], labels_candles_sliced[:s])\n",
    "valid_ds_cnn = ChartImageDataset(paths_to_images[s:], labels_candles_sliced[s:])\n",
    "train_gen_cnn = DataLoader(train_ds_cnn, **params)\n",
    "valid_gen_cnn = DataLoader(valid_ds_cnn, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train(train_gen, model, optim, error_func):\n",
    "    losses = []\n",
    "    \n",
    "    for batch, labels in train_gen:\n",
    "        batch, labels = batch.cuda().float(), labels.cuda().float()\n",
    "        # set model to train mode\n",
    "        model.train()\n",
    "        \n",
    "        # clear gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        output = model(batch)\n",
    "        loss = error_func(output, labels)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        \n",
    "        losses.append(loss)\n",
    "        \n",
    "    return round(float(sum(losses) / len(losses)), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _valid(valid_gen, model, optim, error_func):\n",
    "    with torch.set_grad_enabled(False):\n",
    "        losses = []\n",
    "\n",
    "        for batch, labels in valid_gen:\n",
    "            batch, labels = batch.cuda().float(), labels.cuda().float()\n",
    "            \n",
    "            # set to eval mode\n",
    "            model.eval()\n",
    "            \n",
    "            # clear gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            output = model(batch)\n",
    "            loss = error_func(output, labels)\n",
    "\n",
    "            losses.append(loss)\n",
    "        \n",
    "    return round(float(sum(losses) / len(losses)), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test(test_gen, model, optim, error_func):\n",
    "    with torch.set_grad_enabled(False):\n",
    "        losses = []\n",
    "\n",
    "        for batch, labels in valid_gen:\n",
    "            batch, labels = batch.cuda().float(), labels.cuda().float()\n",
    "            \n",
    "            # set to eval mode\n",
    "            model.eval()\n",
    "            \n",
    "            # clear gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            output = model(batch)\n",
    "            loss = error_func(output, labels)\n",
    "\n",
    "            losses.append(loss)\n",
    "        \n",
    "    return round(float(sum(losses) / len(losses)), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, model_name, optim, num_epochs, train_gen, valid_gen, test_gen=None):\n",
    "    \"\"\"Train a PyTorch model with optim as optimizer strategy\"\"\"\n",
    "    \n",
    "    for epoch_i in range(num_epochs):\n",
    "        \n",
    "        \n",
    "        def RMSE(x, y):\n",
    "            \n",
    "            # have to squish x into a rank 1 tensor with batch_size length with the outputs we want\n",
    "            if model_name == 'resnet':\n",
    "                 # torch.Size([64, 1])\n",
    "                x = x.squeeze(1)\n",
    "            elif model_name == 'gru':\n",
    "                # torch.Size([64, 30, 1])\n",
    "                x = x[:, 29, :] # take only the last prediction from the 30 time periods in our matrix\n",
    "                x = x.squeeze(1)\n",
    "            mse = torch.nn.MSELoss()\n",
    "            return torch.sqrt(mse(x, y))\n",
    "        \n",
    "        \n",
    "        # forward and backward passes of all batches inside train_gen\n",
    "        train_loss = _train(train_gen, model, optim, RMSE)\n",
    "        valid_loss = _valid(valid_gen, model, optim, RMSE)\n",
    "        \n",
    "        # run on test set if provided\n",
    "        if test_gen: test_output = _test(test_gen, model, optim)\n",
    "        else: test_output = \"no test selected\"\n",
    "        print(\"train loss: {}, valid loss: {}, test output: {}\".format(train_loss, valid_loss, test_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.CNN.CNN import CNN\n",
    "cnn = CNN().cuda().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.036602, valid loss: 0.008366, test output: no test selected\n",
      "train loss: 0.011465, valid loss: 0.008114, test output: no test selected\n",
      "train loss: 0.019907, valid loss: 0.007244, test output: no test selected\n",
      "train loss: 0.007044, valid loss: 0.009044, test output: no test selected\n",
      "train loss: 0.004957, valid loss: 0.002753, test output: no test selected\n",
      "train loss: 0.011349, valid loss: 0.023379, test output: no test selected\n",
      "train loss: 0.01012, valid loss: 0.010273, test output: no test selected\n",
      "train loss: 0.006491, valid loss: 0.004303, test output: no test selected\n",
      "train loss: 0.005484, valid loss: 0.002646, test output: no test selected\n",
      "train loss: 0.01114, valid loss: 0.001206, test output: no test selected\n",
      "train loss: 0.004712, valid loss: 0.003009, test output: no test selected\n",
      "train loss: 0.004943, valid loss: 0.005468, test output: no test selected\n",
      "train loss: 0.004107, valid loss: 0.005567, test output: no test selected\n",
      "train loss: 0.003993, valid loss: 0.001519, test output: no test selected\n",
      "train loss: 0.00575, valid loss: 0.009097, test output: no test selected\n"
     ]
    }
   ],
   "source": [
    "train(cnn, 'resnet', torch.optim.Adam(cnn.parameters(), 1e-3), 15, train_gen_cnn, valid_gen_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.GRU.GRU import GRUnet\n",
    "gru = GRUnet(num_features=12, num_rows=30, batch_size=64, hidden_size=500).cuda().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.datasets import DFTimeSeriesDataset, ChartImageDataset\n",
    "from torch.utils.data import *\n",
    "# create dataloaders\n",
    "# specify the split between train_df and valid_df from the process of splitting dataset_windows \n",
    "split = 0.7\n",
    "\n",
    "s = int(len(candles_sliced) * 0.7)\n",
    "while s % params['batch_size'] != 0:\n",
    "    s += 1\n",
    "\n",
    "# create two ChartImageDatasets, split by split, for the purpose of creating a DataLoader for the specific model\n",
    "train_ds_gru = DFTimeSeriesDataset(candles_sliced[:s], labels_candles_sliced[:s])\n",
    "valid_ds_gru = DFTimeSeriesDataset(candles_sliced[s:], labels_candles_sliced[s:])\n",
    "train_gen_gru = DataLoader(train_ds_gru, **params)\n",
    "valid_gen_gru = DataLoader(valid_ds_gru, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.005793, valid loss: 0.001791, test output: no test selected\n",
      "train loss: 0.002739, valid loss: 0.001856, test output: no test selected\n",
      "train loss: 0.002253, valid loss: 0.00184, test output: no test selected\n",
      "train loss: 0.002145, valid loss: 0.00184, test output: no test selected\n",
      "train loss: 0.002133, valid loss: 0.001767, test output: no test selected\n",
      "train loss: 0.002064, valid loss: 0.001802, test output: no test selected\n",
      "train loss: 0.002034, valid loss: 0.00172, test output: no test selected\n",
      "train loss: 0.002014, valid loss: 0.001683, test output: no test selected\n",
      "train loss: 0.002039, valid loss: 0.001656, test output: no test selected\n",
      "train loss: 0.002033, valid loss: 0.001691, test output: no test selected\n",
      "train loss: 0.002, valid loss: 0.001828, test output: no test selected\n",
      "train loss: 0.00199, valid loss: 0.001708, test output: no test selected\n",
      "train loss: 0.001977, valid loss: 0.001685, test output: no test selected\n",
      "train loss: 0.001978, valid loss: 0.001715, test output: no test selected\n",
      "train loss: 0.001978, valid loss: 0.001744, test output: no test selected\n",
      "train loss: 0.001963, valid loss: 0.001737, test output: no test selected\n",
      "train loss: 0.001969, valid loss: 0.001807, test output: no test selected\n",
      "train loss: 0.001968, valid loss: 0.001683, test output: no test selected\n",
      "train loss: 0.001958, valid loss: 0.001695, test output: no test selected\n",
      "train loss: 0.001966, valid loss: 0.001696, test output: no test selected\n",
      "train loss: 0.00196, valid loss: 0.001671, test output: no test selected\n",
      "train loss: 0.001979, valid loss: 0.001664, test output: no test selected\n",
      "train loss: 0.00197, valid loss: 0.001729, test output: no test selected\n",
      "train loss: 0.001961, valid loss: 0.001922, test output: no test selected\n",
      "train loss: 0.001969, valid loss: 0.00178, test output: no test selected\n",
      "train loss: 0.001963, valid loss: 0.001702, test output: no test selected\n",
      "train loss: 0.001958, valid loss: 0.00166, test output: no test selected\n",
      "train loss: 0.001963, valid loss: 0.001788, test output: no test selected\n",
      "train loss: 0.001957, valid loss: 0.001681, test output: no test selected\n",
      "train loss: 0.001954, valid loss: 0.001683, test output: no test selected\n",
      "train loss: 0.001965, valid loss: 0.001659, test output: no test selected\n",
      "train loss: 0.001967, valid loss: 0.001678, test output: no test selected\n",
      "train loss: 0.001967, valid loss: 0.001712, test output: no test selected\n",
      "train loss: 0.001975, valid loss: 0.001727, test output: no test selected\n",
      "train loss: 0.00195, valid loss: 0.002106, test output: no test selected\n",
      "train loss: 0.001969, valid loss: 0.00172, test output: no test selected\n",
      "train loss: 0.001957, valid loss: 0.001704, test output: no test selected\n",
      "train loss: 0.001963, valid loss: 0.001664, test output: no test selected\n",
      "train loss: 0.001954, valid loss: 0.001663, test output: no test selected\n",
      "train loss: 0.001952, valid loss: 0.001667, test output: no test selected\n",
      "train loss: 0.001961, valid loss: 0.001738, test output: no test selected\n",
      "train loss: 0.001963, valid loss: 0.001699, test output: no test selected\n",
      "train loss: 0.001954, valid loss: 0.001699, test output: no test selected\n",
      "train loss: 0.001955, valid loss: 0.001791, test output: no test selected\n",
      "train loss: 0.001955, valid loss: 0.001694, test output: no test selected\n",
      "train loss: 0.001954, valid loss: 0.00169, test output: no test selected\n",
      "train loss: 0.001958, valid loss: 0.001678, test output: no test selected\n",
      "train loss: 0.001958, valid loss: 0.001715, test output: no test selected\n",
      "train loss: 0.001958, valid loss: 0.001671, test output: no test selected\n",
      "train loss: 0.001963, valid loss: 0.001669, test output: no test selected\n"
     ]
    }
   ],
   "source": [
    "train(gru, 'gru', torch.optim.Adam(gru.parameters(), 1e-3), 50, train_gen_gru, valid_gen_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
